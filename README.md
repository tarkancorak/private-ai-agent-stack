# Private AI Agent Stack

Ein modularer Stack f√ºr private, generative KI-Agentensysteme ‚Äì containerisiert, lokal betreibbar und gr√∂√ütenteils Open-Source.

Dieser Stack richtet sich an Organisationen und Entwickler:innen, die generative KI unter eigener Kontrolle einsetzen m√∂chten ‚Äì lokal, on-premise oder in einer selbstverwalteten Cloud. Der Stack bietet volle Kontrolle √ºber Datenfl√ºsse und schafft maximale Flexibilit√§t durch offene Standards.

## F√ºr wen ist dieser Stack gedacht?

### Entwickler:innen und Architekt:innen

- Bietet eine modulare, anpassbare Grundlage f√ºr die Entwicklung von KI-Agenten und -Diensten, die sich in bestehende IT-Landschaften integrieren lassen.
- Durch den Einsatz von Standard-APIs und offenen Technologien lassen sich bestehende Systeme wie Datenbanken sowie Gesch√§fts- und Kommunikationsplattformen einfach anbinden.

### Fachbereiche, Innovationsteams und Automatisierer:innen

- F√ºr Teams, die KI-Agenten und Automatisierungen umsetzen m√∂chten, bietet der Stack eine visuelle Herangehensweise ‚Äì auch ohne tiefgreifende Programmierkenntnisse.
- Bestehende Systeme (z.‚ÄØB. CRM, ERP oder Messaging-Plattformen) k√∂nnen eingebunden und durch Agentenlogik gezielt erweitert werden.

### Organisationen mit Fokus auf Datenschutz und Kontrolle

- Der Stack erlaubt die Verarbeitung sensibler Daten innerhalb der eigenen privaten Infrastruktur.
- Die vollst√§ndige Kontrolle √ºber Modelle und Daten erh√∂ht die Sicherheit und minimiert externe Abh√§ngigkeiten.

## Enthaltene Komponenten

- **Ollama**: Inference-Server f√ºr Open-Source-LLMs (z.‚ÄØB. `Llama`, `Mistral`, `DeepSeek`) mit HTTP-API f√ºr Textgenerierung, Embedding und Modellverwaltung.
- **PostgreSQL** mit `pgvector`: Datenbank mit Vektorunterst√ºtzung f√ºr RAG (Retrieval-Augmented Generation) und als persistenter Vektorspeicher f√ºr Agenten mit semantischem Langzeitged√§chtnis.
- **Open WebUI**: Intuitive Weboberfl√§che zur direkten, privaten Interaktion mit LLMs ‚Äì √§hnlich wie ChatGPT.
- **Flowise**: Visuelle Low-Code-Umgebung zum Aufbau und zur Konfiguration von KI-Agenten.
- **n8n**: Workflow-Automatisierungstool f√ºr die Orchestrierung von Agenten, Diensten und Systemintegrationen.

Die folgende Grafik veranschaulicht grob das Zusammenspiel der Komponenten im Stack:

![Private AI Agent Stack](./private-ai-agent-stack.png)

## Voraussetzungen

- [Docker Desktop](https://www.docker.com/products/docker-desktop/) und [Git](https://git-scm.com/downloads) m√ºssen installiert sein.

## Installation

1. **Repository klonen**

```bash
git clone https://github.com/tarkancorak/private-ai-agent-stack.git
cd private-ai-agent-stack
```

2.&nbsp;&nbsp;Docker-Netzwerk erstellen (nur einmal notwendig)

```bash
docker network create ai-network
```

3.&nbsp;&nbsp;Alle Dienste starten

```bash
# macOS / Linux
./start.sh

# Windows
start.bat
```

Alternativ startest du nur die f√ºr dich relevanten Dienste:

```bash
docker compose -f ollama/docker-compose.yaml up -d
docker compose -f openwebui/docker-compose.yaml up -d
docker compose -f postgres/docker-compose.yaml up -d
docker compose -f flowise/docker-compose.yaml up -d
docker compose -f n8n/docker-compose.yaml up -d
```

‚ÑπÔ∏è Open WebUI ben√∂tigt Ollama f√ºr die Ausf√ºhrung von Modellen.

4.&nbsp;&nbsp;Status pr√ºfen

```bash
# macOS / Linux
./status.sh

# Windows
status.bat
```

5.&nbsp;&nbsp;Dienste aufrufen

| Dienst         | Adresse                  | Hinweise                                                                                                                                                                                          |
| -------------- | ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Open WebUI** | <http://localhost:8080>  | Beim ersten Start kann die Initialisierung einige Minuten dauern. Seite ggf. mehrfach neu laden. Modelle k√∂nnen direkt √ºber die Oberfl√§che ausgew√§hlt oder bei Bedarf manuell hinzugef√ºgt werden. |
| **Flowise**    | <http://localhost:3000>  | Die Zugangsdaten (`FLOWISE_USERNAME`, `FLOWISE_PASSWORD`) sind in der Datei `flowise/docker-compose.yaml` konfiguriert.                                                                           |
| **n8n**        | <http://localhost:5678>  | Beim ersten Aufruf wird ein Benutzerkonto erstellt. Ein Lizenzschl√ºssel kann optional hinterlegt werden.                                                                                          |
| **Ollama API** | <http://localhost:11434> | Zentrale `Inference-API`. `Flowise` und `n8n` nutzen sie zur Kommunikation mit den Modellen, w√§hrend `Open WebUI` eine interaktive Chat-Oberfl√§che bereitstellt.                                  |

---

6.&nbsp;&nbsp;Modelle bereitstellen

üîç **Modell ausw√§hlen**

Du kannst ein Modell √ºber die Oberfl√§che von Open WebUI ausw√§hlen oder direkt per Kommandozeile.

Die [Modellbibliothek von Ollama](https://ollama.com/library) bietet eine aktuelle √úbersicht verf√ºgbarer Modelle mit Beschreibung, Lizenz und Modellgr√∂√üe ‚Äì achte insbesondere auf Unterschiede bei Ressourcenverbrauch und Lizenzbedingungen.

üîß Option 1: Modell √ºber Open WebUI laden

- √ñffne die [Einstellungen in Open WebUI](http://localhost:8080/admin/settings)
- Modellname eingeben oder aus der Liste w√§hlen
- Mit einem Klick herunterladen

üíª Option 2: Modell √ºber Kommandozeile laden

1.&nbsp;&nbsp;Container-ID von Ollama ermitteln:

```bash
docker ps
```

2.&nbsp;&nbsp;Shell im Container √∂ffnen:

```bash
docker exec -it <container-id> bash
```

3.&nbsp;&nbsp;Modell laden (z.‚ÄØB. mistral):

```bash
ollama pull mistral
```

## Weiterf√ºhrende Links

- PostgreSQL: <https://www.postgresql.org/>
- pgvector: <https://github.com/pgvector/pgvector>
- Ollama: <https://ollama.com/>
- Open WebUI: <https://github.com/open-webui/open-webui>
- Flowise: <https://flowiseai.com/>
- n8n: <https://n8n.io/>

## Dienste stoppen und neustarten

```bash
# macOS / Linux
./stop.sh    # Stoppt alle laufenden Container des Stacks
./restart.sh # Startet alle Container des Stacks neu

# Windows
stop.bat     # Stoppt alle laufenden Container des Stacks
restart.bat  # Startet alle Container des Stacks neu
```

## ‚ö†Ô∏è **Hinweise zur Sicherheit**

### Lizenzierung und Modellverwendung

Viele Open-Source-Modelle wie Mistral, Llama oder DeepSeek eignen sich gut f√ºr Entwicklung und Tests. F√ºr produktive Anwendungen sollten die jeweiligen Lizenz- und Nutzungshinweise gepr√ºft werden ‚Äì insbesondere im Hinblick auf kommerzielle Nutzung.

Beachte folgende Punkte:

1. Pr√ºfe die Lizenz des Modells und stelle sicher, dass sie mit den geplanten Einsatzszenarien √ºbereinstimmt.
2. Achte auf die Einhaltung von AUPs (z.‚ÄØB. Einschr√§nkungen f√ºr kommerzielle Nutzung).
3. Stelle sicher, dass der Einsatz den geltenden Gesetzen und Vorschriften entspricht (z.‚ÄØB. DSGVO, Urheberrecht, EU AI Act).

Dieser Hinweis dient der Information und stellt keine rechtliche Beratung dar.

### **Produktiver Einsatz**

Dieser Stack ist f√ºr die lokale Entwicklung und das schnelle Prototyping optimiert. Sicherheitsaspekte und Zugriffsrechte wurden bewusst vereinfacht, um den Einstieg zu erleichtern. F√ºr den Einsatz in Produktionsumgebungen sind entsprechende Anpassungen hinsichtlich Sicherheit und Skalierbarkeit erforderlich.

Ollama eignet sich hervorragend f√ºr den lokalen Einsatz und schnelle Entwicklungszyklen. F√ºr erweiterte Anforderungen l√§sst sich Ollama bei Bedarf um Funktionen wie Modellmanagement, Logging, Monitoring, Zugriffsschutz und Deployment-Automatisierung erweitern ‚Äì z.‚ÄØB. mithilfe von [KubeAI](https://www.kubeai.org/how-to/configure-text-generation-models/), einer Open-Source-Erweiterung, die speziell f√ºr den Betrieb von LLMs in Kubernetes-Umgebungen entwickelt wurde.

‚ÑπÔ∏è F√ºr den produktiven Einsatz wird empfohlen, zus√§tzliche Sicherheitsvorkehrungen zu treffen und die Umgebung gem√§√ü Best Practices hinsichtlich Skalierbarkeit und Ausfallsicherheit zu konfigurieren.

## Warum dieser Stack?

Der Stack ist flexibel, skalierbar und l√§sst sich schnell an neue Anforderungen anpassen. Die Kombination aus Low-Code-Plattform und gr√∂√ütenteils Open-Source-basierter Architektur macht den Stack vielseitig einsetzbar und zukunftssicher:

- Schnell einsetzbar ‚Äì ideal f√ºr interne Tests, Proof-of-Concepts und erste agentenbasierte Workflows.
- Modular & portabel ‚Äì alle Komponenten laufen containerisiert und k√∂nnen einzeln skaliert werden.
- Datensouver√§nit√§t - alle Komponenten k√∂nnen vollst√§ndig unter eigener Kontrolle betrieben werden, ohne Abh√§ngigkeit von externen Anbietern.
- F√ºr gemischte Teams - vereint visuelle Entwicklung (Low-Code) mit API-basierten Integrationen f√ºr Entwickler:innen.
